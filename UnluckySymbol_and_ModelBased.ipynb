{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pystan\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import njit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rewards import Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rs = Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "805"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Sam Zorowitz code â€” add citation here!\n",
    "\n",
    "@njit\n",
    "def inv_logit(arr):\n",
    "    \"\"\"Fast inverse logistic function.\"\"\"\n",
    "    return 1. / (1. + np.exp(-arr))\n",
    "\n",
    "@njit\n",
    "def softmax(arr):\n",
    "    \"\"\"Scale-robust softmax function\"\"\"\n",
    "    arr = np.exp(arr - np.max(arr))\n",
    "    return arr / arr.sum()\n",
    "\n",
    "@njit\n",
    "def phi_approx(arr):\n",
    "    '''Elementwise fast approximation of the cumulative unit normal.'''\n",
    "    return inv_logit(0.07056 * arr ** 3 + 1.5976 * arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "SA = 0\n",
    "SB = 1\n",
    "SC = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnluckySymbol(object):\n",
    "    def __init__(self, trials, Rs, a, b, eta):\n",
    "        \n",
    "        self.n_trials = trials\n",
    "        \n",
    "        self.R = Rs\n",
    "        self.state = SA\n",
    "        \n",
    "        # define parameters\n",
    "        self.alpha = a\n",
    "        self.beta = b\n",
    "        self.eta = eta\n",
    "        \n",
    "        # define values\n",
    "        self.MB = np.zeros((2, 2))\n",
    "        \n",
    "        self.rewards = np.zeros(trials)\n",
    "        self.choices = np.zeros((trials, 2))\n",
    "        self.choice1_outcomes = np.zeros(trials)\n",
    "        self.switch = np.zeros(trials)\n",
    "        self.common = np.zeros(trials)\n",
    "        \n",
    "        self.transitions = np.array([\n",
    "            [0.7, 0.3],\n",
    "            [0.3, 0.7]\n",
    "        ])\n",
    "        \n",
    "        self.transition_count = np.zeros((2,2,2))\n",
    "        self.final_state = np.zeros(trials) # 1 = SB, 2 = SC\n",
    "        \n",
    "    def possible_switch(self, choice):\n",
    "        if random.random() < 0.7:\n",
    "            return choice\n",
    "        return 1 - choice\n",
    "    \n",
    "    def update_stay_prob(self, action, t):\n",
    "        self.transition_count[\n",
    "            int(not self.rewards[t-1]),\n",
    "            int(not self.common[t-1]),\n",
    "            int(not (self.choice1_outcomes[t-1] == action))\n",
    "        ] += 1\n",
    "        \n",
    "    def compute_stay_prob(self, transition_count):\n",
    "        # stay_prob[r,c,a] = P[r,c,a] / (P[r,c,a] + P[r,c,~a]) \n",
    "        action_count = transition_count.sum(axis=-1)\n",
    "        return transition_count / action_count[:, :, np.newaxis]\n",
    "\n",
    "    def train(self, R):\n",
    "        \n",
    "        for t in range(self.n_trials):\n",
    "            \n",
    "            self.state = SA\n",
    "            \n",
    "            ## Action selection.\n",
    "            d1 = 0.7*max(self.MB[1]) + 0.3*max(self.MB[0]) - self.eta*(0.3*max(self.MB[1]) + 0.7*max(self.MB[0]))\n",
    "            \n",
    "            # choice probabilities and making choice\n",
    "            theta1 = inv_logit( self.beta * d1 )\n",
    "            choice1 = np.random.binomial(1, theta1)\n",
    "            self.choices[t,0] = choice1\n",
    "            \n",
    "            # observe outcome and possible switch\n",
    "            outcome1 = self.possible_switch(choice1)\n",
    "            self.choice1_outcomes[t] = outcome1\n",
    "            \n",
    "            # update values for stay_probs\n",
    "            if t > 0:    \n",
    "                self.update_stay_prob(outcome1, t)\n",
    "            \n",
    "            reward_probs = None\n",
    "            # update state\n",
    "            if self.choice1_outcomes[t] == 0: # went LEFT\n",
    "                self.state = SB\n",
    "                self.final_state[t] = 1\n",
    "                reward_probs = self.R[t][0:2]\n",
    "            else: # went RIGHT\n",
    "                self.state = SC\n",
    "                self.final_state[t] = 2\n",
    "                reward_probs = self.R[t][2:4]\n",
    "            \n",
    "            # count possible switch\n",
    "            if (self.choices[t,0] == 1 & self.state == SB) | (self.choices[t,0] == 0 & self.state == SC):\n",
    "                self.switch[t] = 1\n",
    "            else:\n",
    "                self.common[t] = 1\n",
    "            \n",
    "            # possible value reduction\n",
    "            value_reduc = eta if (choice1 == 0) else 1\n",
    "            \n",
    "            # make second-level choice\n",
    "            d2 = value_reduc*self.beta*(self.MB[outcome1,1] - self.MB[outcome1,0])\n",
    "\n",
    "            theta2 = inv_logit( d2 )\n",
    "            choice2 = np.random.binomial(1, theta2)\n",
    "            self.choices[t,1] = choice2\n",
    "            \n",
    "            # get what the reward is\n",
    "            final_prob = reward_probs[choice2]\n",
    "            reward = np.random.binomial(1, final_prob)\n",
    "            self.rewards[t] = reward\n",
    "            \n",
    "            # update values\n",
    "            self.MB[outcome1, choice2] = (1 - self.alpha)*self.MB[outcome1, choice2] + self.alpha*self.rewards[t]\n",
    "\n",
    "            \n",
    "    def plot(self, transition_count=None, title=\"Unlucky Symbol: Two-Step Task\", y_lim=0.5):\n",
    "        _,ax = plt.subplots(1,1,figsize=[10,6])\n",
    "\n",
    "        ax.set_ylim([y_lim, 1.0])\n",
    "        ax.set_ylabel('Stay Probability')\n",
    "        ax.set_title(title)\n",
    "\n",
    "        if transition_count is None:\n",
    "            transition_count = self.transition_count\n",
    "        \n",
    "        stay_probs = self.compute_stay_prob(transition_count)\n",
    "        \n",
    "        common = [stay_probs[0,0,0], stay_probs[1,0,0]]\n",
    "        uncommon = [stay_probs[0,1,0], stay_probs[1,1,0]]\n",
    "        \n",
    "        ax.set_xticks([1.5,3.5])\n",
    "        ax.set_xticklabels(['Rewarded', 'Unrewarded'])\n",
    "        ax.set_ylim(0,1)\n",
    "        \n",
    "        c = plt.bar([1,3], common, color='b', width=0.5)\n",
    "        uc = plt.bar([2,4], uncommon, color='r', width=0.5)\n",
    "        ax.legend( (c[0], uc[0]), ('Common', 'Uncommon') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_agents = 1\n",
    "trials = 400\n",
    "alpha = 0.5\n",
    "beta = 5.00\n",
    "eta  = 0.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = UnluckySymbol(trials, Rs, alpha, beta, eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(47404)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = agents.train(Rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2., 1., 2., 2., 2., 2., 2., 2., 2., 2., 1., 1., 2., 1., 2., 2.,\n",
       "       1., 2., 2., 2., 1., 1., 2., 1., 2., 1., 2., 2., 1., 1., 1., 1., 1.,\n",
       "       2., 2., 1., 1., 2., 2., 2., 1., 2., 2., 1., 2., 1., 2., 2., 1., 2.,\n",
       "       2., 1., 2., 2., 1., 1., 2., 2., 1., 2., 2., 2., 2., 1., 1., 2., 2.,\n",
       "       1., 2., 1., 2., 1., 2., 2., 2., 2., 2., 1., 1., 1., 2., 2., 2., 1.,\n",
       "       2., 2., 2., 1., 1., 2., 1., 2., 1., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "       2., 2., 2., 1., 2., 1., 2., 2., 2., 1., 1., 1., 2., 1., 2., 1., 2.,\n",
       "       2., 2., 1., 2., 2., 1., 1., 1., 2., 2., 1., 1., 2., 1., 1., 2., 2.,\n",
       "       2., 1., 1., 2., 2., 2., 2., 2., 2., 1., 1., 2., 2., 2., 2., 1., 2.,\n",
       "       2., 2., 2., 1., 1., 2., 2., 2., 2., 1., 2., 1., 2., 2., 1., 2., 2.,\n",
       "       2., 2., 1., 2., 2., 2., 1., 1., 2., 2., 2., 1., 1., 1., 2., 1., 2.,\n",
       "       2., 2., 2., 2., 2., 1., 2., 1., 2., 2., 1., 2., 2., 2., 2., 1., 2.,\n",
       "       2., 1., 1., 1., 2., 2., 2., 2., 2., 2., 1., 1., 2., 2., 2., 1., 2.,\n",
       "       1., 2., 1., 2., 1., 2., 1., 2., 2., 2., 2., 1., 2., 2., 2., 2., 1.,\n",
       "       2., 2., 2., 1., 2., 2., 2., 2., 1., 1., 1., 1., 2., 2., 2., 2., 2.,\n",
       "       1., 1., 2., 2., 2., 2., 2., 2., 2., 2., 1., 1., 2., 2., 2., 2., 2.,\n",
       "       2., 2., 1., 1., 1., 1., 2., 2., 1., 2., 2., 2., 2., 2., 1., 1., 2.,\n",
       "       1., 1., 1., 2., 2., 2., 2., 2., 1., 2., 2., 1., 2., 2., 2., 1., 1.,\n",
       "       2., 1., 2., 2., 2., 1., 2., 1., 2., 1., 2., 1., 2., 2., 1., 1., 2.,\n",
       "       2., 1., 2., 1., 1., 1., 1., 2., 1., 1., 1., 1., 1., 1., 1., 1., 2.,\n",
       "       1., 2., 1., 1., 2., 2., 1., 2., 1., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "       2., 1., 2., 2., 1., 2., 2., 1., 2., 1., 2., 2., 1., 1., 1., 2., 1.,\n",
       "       1., 2., 1., 2., 2., 1., 1., 1., 1., 2., 1., 2., 2., 2., 2., 1., 1.,\n",
       "       2., 2., 2., 2., 2., 2., 2., 2., 1.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agents.final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rew = agents.rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rares = agents.switch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "commons = agents.common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking for HIGHER STAY PROBABILITY:\n",
    "    # if common and was rewarded ()\n",
    "    # or uncommon and wasn't rewarded (switch, and no reward)\n",
    "\n",
    "com_rew = []\n",
    "com_unrew = []\n",
    "\n",
    "rare_unrew = []\n",
    "rare_rew = []\n",
    "    \n",
    "for i in range(trials):\n",
    "    if (commons[i] == 1):\n",
    "        com_rew.append(i) if (rew[i] == 1) else com_unrew.append(i)\n",
    "\n",
    "    if (rares[i] == 1):\n",
    "        rare_unrew.append(i) if (rew[i] == 0) else rare_rew.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(com_rew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rare_unrew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rare_rew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(com_unrew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "agents.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [EDIT] Sam Notes\n",
    "\n",
    "In this cell, I'll review the equations for the model-based agent, originally defined in the [supplement of Daw et al (2011)](https://ars.els-cdn.com/content/image/1-s2.0-S0896627311001255-mmc1.pdf). \n",
    "\n",
    "#### Stage 2 Choice\n",
    "\n",
    "In stage 2, the agent makes a choice between two actions, $Q(s_i, a_1)$ and $Q(s_i,a_2$), conditioned on the current state. As discussed in [Daw (2011)](https://pdfs.semanticscholar.org/43c3/d7653710bbb477df108fc2ed2729429d053c.pdf), in the case of two options, the softmax reduces to the [inverse logit function](https://en.wikipedia.org/wiki/Logistic_function). Thus, the probability of the agent taking action 1 is:\n",
    "\n",
    "$$ p( a_1 \\mid s_i, \\beta_2 ) = \\text{logit}^{-1} \\left( \\beta_2 \\cdot [Q(s_i, a_1) - Q(s_i, a_2)] \\right) = \\frac{1}{1 + \\exp \\left(-\\beta_2 \\cdot [Q(s_i, a_1) - Q(s_i, a_2)] \\right)} $$\n",
    "\n",
    "Note that the action probability is a function of a choice sensitivity (inverse temperature) parameter, $\\beta_2$. The subscript indicates that this parameter is specific to the second stage. As you will see below, we introduce a separate choice sensitivity parameter for first stage choice (even though we typically set these to the same value).\n",
    "\n",
    "#### Stage 2 Learning\n",
    "\n",
    "In stage 2, following a choice, the agent updates its expectations of the value of the chosen action, $Q(s_{i,t+1} a_{i,t+1})$, based on the observed reward, $r_t$. This update folows temporal difference learning:\n",
    "\n",
    "$$ Q(s_{i,t+1} a_{i,t+1}) = Q(s_{i,t}, a_{i,t}) + \\eta_2 \\delta_t $$\n",
    "\n",
    "where $\\delta_t$ is the reward prediction error on trial $t$, defined as:\n",
    "\n",
    "$$ \\delta_t = r_t - Q(s_{i,t}, a_{i,t}) $$\n",
    "\n",
    "Two notes here:\n",
    "\n",
    "1. We introduce the trial $t$ notation here to denote that learning updates affect behavior on the next trial.\n",
    "2. We introduce the stage notation here for learning rate to indicate that this parameter is specific to stage 2.\n",
    "\n",
    "#### Stage 1 Choice\n",
    "\n",
    "Now here's the complicated part. We make two assumptions about the model-based agent:\n",
    "\n",
    "1. The agent recomputes the value of each stage 1 action based on its current estimates of the value of the second stage choices at the start of each trial.\n",
    "2. The agent performs this computation assuming an *off-policy* strategy (i.e. assuming it will take the best action at any successor state).\n",
    "\n",
    "Following Bellman's equation, the model-based agent computes the value of each first stage action taking into consideration the state-transition probaiblities, $T(s' \\mid s_1, a_i)$, and value of the best action in each successor state. \n",
    "\n",
    "For notational convenience, let's define the transition probabilities as:\n",
    "- $ p( s_2 \\mid s_1, a_1) = 0.7 $, i.e. 70% probability of ending up in State 2 given Action 1 in State 1\n",
    "- $ p( s_3 \\mid s_1, a_1) = 0.3 $, i.e. 30% probability of ending up in State 3 given Action 1 in State 1\n",
    "- $ p( s_2 \\mid s_1, a_2) = 0.3 $, i.e. 30% probability of ending up in State 2 given Action 2 in State 1\n",
    "- $ p( s_3 \\mid s_1, a_2) = 0.7 $, i.e. 70% probability of ending up in State 3 given Action 2 in State 1\n",
    "\n",
    "Then we can define the value of action 1 in stage 1 as:\n",
    "\n",
    "$$ Q(s_1, a_1) = p( s_2 \\mid s_1, a_1) \\cdot \\max Q(s_2, a) + p( s_3 \\mid s_1, a_1) \\cdot \\max Q(s_3, a) $$\n",
    "$$ = 0.7 \\cdot \\max Q(s_2, a) + 0.3 \\cdot \\max Q(s_3, a) $$\n",
    "\n",
    "and the value of action 2 in stage 1 as:\n",
    "\n",
    "$$ Q(s_1, a_2) = p( s_2 \\mid s_1, a_2) \\cdot \\max Q(s_2, a) + p( s_3 \\mid s_1, a_2) \\cdot \\max Q(s_3, a) $$\n",
    "$$ = 0.3 \\cdot \\max Q(s_2, a) + 0.7 \\cdot \\max Q(s_3, a) $$\n",
    "\n",
    "And finally, we use the inverse logit function to assign probabilities of taking either action 1 vs. action 2 in stage 1:\n",
    "\n",
    "$$ p( a_1 \\mid s_1, \\beta_1 ) = \\text{logit}^{-1} \\left( \\beta_1 \\cdot [Q(s_1, a_1) - Q(s_1, a_2)] \\right) = \\frac{1}{1 + \\exp \\left(-\\beta_1 \\cdot [Q(s_1, a_1) - Q(s_1, a_2)] \\right)} $$\n",
    "\n",
    "\n",
    "Note that the action probability is a function of a separate choice sensitivity (inverse temperature) parameter, $\\beta_1$. The subscript indicates that this parameter is specific to the first stage. As noted above, we typically set the value of choice sensitivity for both stages, $\\beta_1$ and $\\beta_2$, to be the same but they don't have to be!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model-Based Strategy\n",
    "\n",
    "class model_based(object):\n",
    "    def __init__(self, trials, Rs, a, b1, b2=None):\n",
    "        \n",
    "        self.n_trials = trials\n",
    " \n",
    "        self.R = Rs\n",
    "        self.state = SA\n",
    "        \n",
    "        # define parameters\n",
    "        self.alpha = a\n",
    "        self.beta1 = b1\n",
    "        \n",
    "        if b2 == None:\n",
    "            self.beta2 = b1\n",
    "        else:\n",
    "            self.beta2 = b2\n",
    "        \n",
    "        # define values\n",
    "        self.MB = np.zeros((2, 2))\n",
    "        \n",
    "        self.rewards = np.zeros(trials)\n",
    "        self.choices = np.zeros((trials, 2))\n",
    "        self.choice1_outcomes = np.zeros(trials)\n",
    "        self.switch = np.zeros(trials)\n",
    "        self.common = np.zeros(trials)\n",
    "        \n",
    "        self.transitions = np.array([\n",
    "            [0.7, 0.3],\n",
    "            [0.3, 0.7]\n",
    "        ])\n",
    "        \n",
    "        self.transition_count = np.zeros((2,2,2))\n",
    "        self.final_state = np.zeros(trials) # 1 = SB, 2 = SC\n",
    "        \n",
    "    def possible_switch(self, choice):\n",
    "        if random.random() < 0.7:\n",
    "            return choice\n",
    "        return 1 - choice\n",
    "    \n",
    "    def update_stay_prob(self, action, t):\n",
    "        self.transition_count[\n",
    "            int(not self.rewards[t-1]),\n",
    "            int(not self.common[t-1]),\n",
    "            int(not (self.choice1_outcomes[t-1] == action))\n",
    "        ] += 1\n",
    "        \n",
    "    def compute_stay_prob(self, transition_count):\n",
    "        # stay_prob[r,c,a] = P[r,c,a] / (P[r,c,a] + P[r,c,~a]) \n",
    "        action_count = transition_count.sum(axis=-1)\n",
    "        return transition_count / action_count[:, :, np.newaxis]\n",
    "\n",
    "    def train(self, R):\n",
    "        \n",
    "        for t in range(self.n_trials):\n",
    "            \n",
    "            self.state = SA\n",
    "            \n",
    "            ## Action selection.\n",
    "            v1 = 0.7*max(self.MB[0]) + 0.3*max(self.MB[1])\n",
    "            v2 = 0.3*max(self.MB[0]) + 0.7*max(self.MB[1])\n",
    "\n",
    "            # choice probabilities and making choice\n",
    "            theta1 = inv_logit( self.beta1 * (v1 - v2) )\n",
    "            choice1 = np.random.binomial(1, theta1)\n",
    "            self.choices[t,0] = choice1\n",
    "            \n",
    "            # observe outcome and possible switch\n",
    "            outcome1 = self.possible_switch(choice1)\n",
    "            self.choice1_outcomes[t] = outcome1\n",
    "            \n",
    "            # update values for stay_probs\n",
    "            if t > 0:    \n",
    "                self.update_stay_prob(outcome1, t)\n",
    "            \n",
    "            reward_probs = None\n",
    "            # update state\n",
    "            if self.choice1_outcomes[t] == 0: # went LEFT\n",
    "                self.state = SB\n",
    "                self.final_state[t] = 1\n",
    "                reward_probs = self.R[t][0:2]\n",
    "            else: # went RIGHT\n",
    "                self.state = SC\n",
    "                self.final_state[t] = 2\n",
    "                reward_probs = self.R[t][2:4]\n",
    "            \n",
    "            # count possible switch\n",
    "            if (self.choices[t,0] == 1 & self.state == SB) | (self.choices[t,0] == 0 & self.state == SC):\n",
    "                self.switch[t] = 1\n",
    "            else:\n",
    "                self.common[t] = 1\n",
    "            \n",
    "            # make second-level choice\n",
    "            d2 = self.MB[outcome1,0] - self.MB[outcome1,1]\n",
    "\n",
    "            theta2 = inv_logit( self.beta2 * d2 )\n",
    "            choice2 = np.random.binomial(1, theta2)\n",
    "            self.choices[t,1] = choice2\n",
    "            \n",
    "            # get what the reward is\n",
    "            final_prob = reward_probs[choice2]\n",
    "            reward = np.random.binomial(1, final_prob)\n",
    "            self.rewards[t] = reward\n",
    "            \n",
    "            # update values\n",
    "            # update level 2 MB value\n",
    "            self.MB[outcome1, choice2] += self.alpha*(self.rewards[t] - self.MB[outcome1, choice2])\n",
    "            \n",
    "    def plot(self, transition_count=None, title=\"Model-Based Two-Step Task\", y_lim=0.5):\n",
    "        _,ax = plt.subplots(1,1,figsize=[10,6])\n",
    "\n",
    "        ax.set_ylim([y_lim, 1.0])\n",
    "        ax.set_ylabel('Stay Probability')\n",
    "        ax.set_title(title)\n",
    "\n",
    "        if transition_count is None:\n",
    "            transition_count = self.transition_count\n",
    "        \n",
    "        stay_probs = self.compute_stay_prob(transition_count)\n",
    "        \n",
    "        common = [stay_probs[0,0,0], stay_probs[1,0,0]]\n",
    "        uncommon = [stay_probs[0,1,0], stay_probs[1,1,0]]\n",
    "        \n",
    "        ax.set_xticks([1.5,3.5])\n",
    "        ax.set_xticklabels(['Rewarded', 'Unrewarded'])\n",
    "        ax.set_ylim(0,1)\n",
    "        \n",
    "        c = plt.bar([1,3], common, color='b', width=0.5)\n",
    "        uc = plt.bar([2,4], uncommon, color='r', width=0.5)\n",
    "        ax.legend( (c[0], uc[0]), ('Common', 'Uncommon') )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_agents = 1\n",
    "trials = 400\n",
    "alpha = 0.5\n",
    "beta = 10.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(11408)\n",
    "agents_MB = model_based(trials, Rs, alpha, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "MBtrained = agents_MB.train(Rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "agents_MB.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StayProbabilitiesCalculator:\n",
    "    \"\"\"Helper class for the stay probabilities.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.prev_reward = None\n",
    "        self.prev_transition = None\n",
    "        self.prev_choice = None\n",
    "        self.stay = [(0, 0) for _ in range(4)]\n",
    "    def add_trial_info(self, choice1, fstate, _, reward):\n",
    "        if self.prev_choice is not None:\n",
    "            indx = 2*self.prev_reward + self.prev_transition\n",
    "            num, den = self.stay[indx]\n",
    "            self.stay[indx] = (num + int(self.prev_choice == choice1), den + 1)\n",
    "        self.prev_reward = reward\n",
    "        self.prev_transition = int(choice1 == fstate)\n",
    "        self.prev_choice = choice1\n",
    "    def get_stay_prob(self, reward, transition):\n",
    "        num, den = self.stay[2*reward + transition]\n",
    "        return num/den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "spc = StayProbabilitiesCalculator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "choice1 = agents_MB.choices[:,0]\n",
    "fstate = agents_MB.choice1_outcomes\n",
    "choice2 = agents_MB.choices[:,1]\n",
    "reward= agents_MB.rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS WRONG!! â€”â€” something wrong here with the way that you are coding the task . . . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "# would this be better if we used two betas, one for each stage, like Daw and Toyama did?\n",
    "# is this the right way to calculate Stay Probabilities? This seems low when you look at the graph . . . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
