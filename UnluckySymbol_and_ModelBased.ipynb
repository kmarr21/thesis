{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pystan\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import njit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rewards import Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rs = Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "805"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Sam Zorowitz code â€” add citation here!\n",
    "\n",
    "@njit\n",
    "def inv_logit(arr):\n",
    "    \"\"\"Fast inverse logistic function.\"\"\"\n",
    "    return 1. / (1. + np.exp(-arr))\n",
    "\n",
    "@njit\n",
    "def softmax(arr):\n",
    "    \"\"\"Scale-robust softmax function\"\"\"\n",
    "    arr = np.exp(arr - np.max(arr))\n",
    "    return arr / arr.sum()\n",
    "\n",
    "@njit\n",
    "def phi_approx(arr):\n",
    "    '''Elementwise fast approximation of the cumulative unit normal.'''\n",
    "    return inv_logit(0.07056 * arr ** 3 + 1.5976 * arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "SA = 0\n",
    "SB = 1\n",
    "SC = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnluckySymbol(object):\n",
    "    def __init__(self, trials, Rs, a, b, eta):\n",
    "        \n",
    "        self.n_trials = trials\n",
    "        \n",
    "        self.R = Rs\n",
    "        self.state = SA\n",
    "        \n",
    "        # define parameters\n",
    "        self.alpha = a\n",
    "        self.beta = b\n",
    "        self.eta = eta\n",
    "        \n",
    "        # define values\n",
    "        self.MB = np.zeros((2, 2))\n",
    "        \n",
    "        self.rewards = np.zeros(trials)\n",
    "        self.choices = np.zeros((trials, 2))\n",
    "        self.choice1_outcomes = np.zeros(trials)\n",
    "        self.switch = np.zeros(trials)\n",
    "        self.common = np.zeros(trials)\n",
    "        \n",
    "        self.transitions = np.array([\n",
    "            [0.7, 0.3],\n",
    "            [0.3, 0.7]\n",
    "        ])\n",
    "        \n",
    "        self.transition_count = np.zeros((2,2,2))\n",
    "        self.final_state = np.zeros(trials) # 1 = SB, 2 = SC\n",
    "        \n",
    "    def possible_switch(self, choice):\n",
    "        if random.random() < 0.7:\n",
    "            return choice\n",
    "        return 1 - choice\n",
    "    \n",
    "    def update_stay_prob(self, action, t):\n",
    "        self.transition_count[\n",
    "            int(not self.rewards[t-1]),\n",
    "            int(not self.common[t-1]),\n",
    "            int(not (self.choice1_outcomes[t-1] == action))\n",
    "        ] += 1\n",
    "        \n",
    "    def compute_stay_prob(self, transition_count):\n",
    "        # stay_prob[r,c,a] = P[r,c,a] / (P[r,c,a] + P[r,c,~a]) \n",
    "        action_count = transition_count.sum(axis=-1)\n",
    "        return transition_count / action_count[:, :, np.newaxis]\n",
    "\n",
    "    def train(self, R):\n",
    "        \n",
    "        for t in range(self.n_trials):\n",
    "            \n",
    "            self.state = SA\n",
    "            \n",
    "            ## Action selection.\n",
    "            d1 = 0.7*max(self.MB[1]) + 0.3*max(self.MB[0]) - self.eta*(0.3*max(self.MB[1]) + 0.7*max(self.MB[0]))\n",
    "            \n",
    "            # choice probabilities and making choice\n",
    "            theta1 = inv_logit( self.beta * d1 )\n",
    "            choice1 = np.random.binomial(1, theta1)\n",
    "            self.choices[t,0] = choice1\n",
    "            \n",
    "            # observe outcome and possible switch\n",
    "            outcome1 = self.possible_switch(choice1)\n",
    "            self.choice1_outcomes[t] = outcome1\n",
    "            \n",
    "            # update values for stay_probs\n",
    "            if t > 0:    \n",
    "                self.update_stay_prob(outcome1, t)\n",
    "            \n",
    "            reward_probs = None\n",
    "            # update state\n",
    "            if self.choice1_outcomes[t] == 0: # went LEFT\n",
    "                self.state = SB\n",
    "                self.final_state[t] = 1\n",
    "                reward_probs = self.R[t][0:2]\n",
    "            else: # went RIGHT\n",
    "                self.state = SC\n",
    "                self.final_state[t] = 2\n",
    "                reward_probs = self.R[t][2:4]\n",
    "            \n",
    "            # count possible switch\n",
    "            if (self.choices[t,0] == 1 & self.state == SB) | (self.choices[t,0] == 0 & self.state == SC):\n",
    "                self.switch[t] = 1\n",
    "            else:\n",
    "                self.common[t] = 1\n",
    "            \n",
    "            # possible value reduction\n",
    "            value_reduc = eta if (choice1 == 0) else 1\n",
    "            \n",
    "            # make second-level choice\n",
    "            d2 = value_reduc*self.beta*(self.MB[outcome1,1] - self.MB[outcome1,0])\n",
    "\n",
    "            theta2 = inv_logit( d2 )\n",
    "            choice2 = np.random.binomial(1, theta2)\n",
    "            self.choices[t,1] = choice2\n",
    "            \n",
    "            # get what the reward is\n",
    "            final_prob = reward_probs[choice2]\n",
    "            reward = np.random.binomial(1, final_prob)\n",
    "            self.rewards[t] = reward\n",
    "            \n",
    "            # update values\n",
    "            self.MB[outcome1, choice2] = (1 - self.alpha)*self.MB[outcome1, choice2] + self.alpha*self.rewards[t]\n",
    "\n",
    "            \n",
    "    def plot(self, transition_count=None, title=\"Unlucky Symbol: Two-Step Task\", y_lim=0.5):\n",
    "        _,ax = plt.subplots(1,1,figsize=[10,6])\n",
    "\n",
    "        ax.set_ylim([y_lim, 1.0])\n",
    "        ax.set_ylabel('Stay Probability')\n",
    "        ax.set_title(title)\n",
    "\n",
    "        if transition_count is None:\n",
    "            transition_count = self.transition_count\n",
    "        \n",
    "        stay_probs = self.compute_stay_prob(transition_count)\n",
    "        \n",
    "        common = [stay_probs[0,0,0], stay_probs[1,0,0]]\n",
    "        uncommon = [stay_probs[0,1,0], stay_probs[1,1,0]]\n",
    "        \n",
    "        ax.set_xticks([1.5,3.5])\n",
    "        ax.set_xticklabels(['Rewarded', 'Unrewarded'])\n",
    "        ax.set_ylim(0,1)\n",
    "        \n",
    "        c = plt.bar([1,3], common, color='b', width=0.5)\n",
    "        uc = plt.bar([2,4], uncommon, color='r', width=0.5)\n",
    "        ax.legend( (c[0], uc[0]), ('Common', 'Uncommon') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_agents = 1\n",
    "trials = 400\n",
    "alpha = 0.5\n",
    "beta = 5.00\n",
    "eta  = 0.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = UnluckySymbol(trials, Rs, alpha, beta, eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(47404)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = agents.train(Rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2., 1., 2., 2., 2., 2., 2., 2., 2., 2., 1., 1., 2., 1., 2., 2.,\n",
       "       1., 2., 2., 2., 1., 1., 2., 1., 2., 1., 2., 2., 1., 1., 1., 1., 1.,\n",
       "       2., 2., 1., 1., 2., 2., 2., 1., 2., 2., 1., 2., 1., 2., 2., 1., 2.,\n",
       "       2., 1., 2., 2., 1., 1., 2., 2., 1., 2., 2., 2., 2., 1., 1., 2., 2.,\n",
       "       1., 2., 1., 2., 1., 2., 2., 2., 2., 2., 1., 1., 1., 2., 2., 2., 1.,\n",
       "       2., 2., 2., 1., 1., 2., 1., 2., 1., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "       2., 2., 2., 1., 2., 1., 2., 2., 2., 1., 1., 1., 2., 1., 2., 1., 2.,\n",
       "       2., 2., 1., 2., 2., 1., 1., 1., 2., 2., 1., 1., 2., 1., 1., 2., 2.,\n",
       "       2., 1., 1., 2., 2., 2., 2., 2., 2., 1., 1., 2., 2., 2., 2., 1., 2.,\n",
       "       2., 2., 2., 1., 1., 2., 2., 2., 2., 1., 2., 1., 2., 2., 1., 2., 2.,\n",
       "       2., 2., 1., 2., 2., 2., 1., 1., 2., 2., 2., 1., 1., 1., 2., 1., 2.,\n",
       "       2., 2., 2., 2., 2., 1., 2., 1., 2., 2., 1., 2., 2., 2., 2., 1., 2.,\n",
       "       2., 1., 1., 1., 2., 2., 2., 2., 2., 2., 1., 1., 2., 2., 2., 1., 2.,\n",
       "       1., 2., 1., 2., 1., 2., 1., 2., 2., 2., 2., 1., 2., 2., 2., 2., 1.,\n",
       "       2., 2., 2., 1., 2., 2., 2., 2., 1., 1., 1., 1., 2., 2., 2., 2., 2.,\n",
       "       1., 1., 2., 2., 2., 2., 2., 2., 2., 2., 1., 1., 2., 2., 2., 2., 2.,\n",
       "       2., 2., 1., 1., 1., 1., 2., 2., 1., 2., 2., 2., 2., 2., 1., 1., 2.,\n",
       "       1., 1., 1., 2., 2., 2., 2., 2., 1., 2., 2., 1., 2., 2., 2., 1., 1.,\n",
       "       2., 1., 2., 2., 2., 1., 2., 1., 2., 1., 2., 1., 2., 2., 1., 1., 2.,\n",
       "       2., 1., 2., 1., 1., 1., 1., 2., 1., 1., 1., 1., 1., 1., 1., 1., 2.,\n",
       "       1., 2., 1., 1., 2., 2., 1., 2., 1., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "       2., 1., 2., 2., 1., 2., 2., 1., 2., 1., 2., 2., 1., 1., 1., 2., 1.,\n",
       "       1., 2., 1., 2., 2., 1., 1., 1., 1., 2., 1., 2., 2., 2., 2., 1., 1.,\n",
       "       2., 2., 2., 2., 2., 2., 2., 2., 1.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agents.final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rew = agents.rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rares = agents.switch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "commons = agents.common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking for HIGHER STAY PROBABILITY:\n",
    "    # if common and was rewarded ()\n",
    "    # or uncommon and wasn't rewarded (switch, and no reward)\n",
    "\n",
    "com_rew = []\n",
    "com_unrew = []\n",
    "\n",
    "rare_unrew = []\n",
    "rare_rew = []\n",
    "    \n",
    "for i in range(trials):\n",
    "    if (commons[i] == 1):\n",
    "        com_rew.append(i) if (rew[i] == 1) else com_unrew.append(i)\n",
    "\n",
    "    if (rares[i] == 1):\n",
    "        rare_unrew.append(i) if (rew[i] == 0) else rare_rew.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(com_rew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rare_unrew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rare_rew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(com_unrew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "agents.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [EDIT] Sam Notes\n",
    "\n",
    "In this cell, I'll review the equations for the model-based agent, originally defined in the [supplement of Daw et al (2011)](https://ars.els-cdn.com/content/image/1-s2.0-S0896627311001255-mmc1.pdf). \n",
    "\n",
    "#### Stage 2 Choice\n",
    "\n",
    "In stage 2, the agent makes a choice between two actions, $Q(s_i, a_1)$ and $Q(s_i,a_2$), conditioned on the current state. As discussed in [Daw (2011)](https://pdfs.semanticscholar.org/43c3/d7653710bbb477df108fc2ed2729429d053c.pdf), in the case of two options, the softmax reduces to the [inverse logit function](https://en.wikipedia.org/wiki/Logistic_function). Thus, the probability of the agent taking action 1 is:\n",
    "\n",
    "$$ p( a_1 \\mid s_i, \\beta_2 ) = \\text{logit}^{-1} \\left( \\beta_2 \\cdot [Q(s_i, a_1) - Q(s_i, a_2)] \\right) = \\frac{1}{1 + \\exp \\left(-\\beta_2 \\cdot [Q(s_i, a_1) - Q(s_i, a_2)] \\right)} $$\n",
    "\n",
    "Note that the action probability is a function of a choice sensitivity (inverse temperature) parameter, $\\beta_2$. The subscript indicates that this parameter is specific to the second stage. As you will see below, we introduce a separate choice sensitivity parameter for first stage choice (even though we typically set these to the same value).\n",
    "\n",
    "#### Stage 2 Learning\n",
    "\n",
    "In stage 2, following a choice, the agent updates its expectations of the value of the chosen action, $Q(s_{i,t+1} a_{i,t+1})$, based on the observed reward, $r_t$. This update folows temporal difference learning:\n",
    "\n",
    "$$ Q(s_{i,t+1} a_{i,t+1}) = Q(s_{i,t}, a_{i,t}) + \\eta_2 \\delta_t $$\n",
    "\n",
    "where $\\delta_t$ is the reward prediction error on trial $t$, defined as:\n",
    "\n",
    "$$ \\delta_t = r_t - Q(s_{i,t}, a_{i,t}) $$\n",
    "\n",
    "Two notes here:\n",
    "\n",
    "1. We introduce the trial $t$ notation here to denote that learning updates affect behavior on the next trial.\n",
    "2. We introduce the stage notation here for learning rate to indicate that this parameter is specific to stage 2.\n",
    "\n",
    "#### Stage 1 Choice\n",
    "\n",
    "Now here's the complicated part. We make two assumptions about the model-based agent:\n",
    "\n",
    "1. The agent recomputes the value of each stage 1 action based on its current estimates of the value of the second stage choices at the start of each trial.\n",
    "2. The agent performs this computation assuming an *off-policy* strategy (i.e. assuming it will take the best action at any successor state).\n",
    "\n",
    "Following Bellman's equation, the model-based agent computes the value of each first stage action taking into consideration the state-transition probaiblities, $T(s' \\mid s_1, a_i)$, and value of the best action in each successor state. \n",
    "\n",
    "For notational convenience, let's define the transition probabilities as:\n",
    "- $ p( s_2 \\mid s_1, a_1) = 0.7 $, i.e. 70% probability of ending up in State 2 given Action 1 in State 1\n",
    "- $ p( s_3 \\mid s_1, a_1) = 0.3 $, i.e. 30% probability of ending up in State 3 given Action 1 in State 1\n",
    "- $ p( s_2 \\mid s_1, a_2) = 0.3 $, i.e. 30% probability of ending up in State 2 given Action 2 in State 1\n",
    "- $ p( s_3 \\mid s_1, a_2) = 0.7 $, i.e. 70% probability of ending up in State 3 given Action 2 in State 1\n",
    "\n",
    "Then we can define the value of action 1 in stage 1 as:\n",
    "\n",
    "$$ Q(s_1, a_1) = p( s_2 \\mid s_1, a_1) \\cdot \\max Q(s_2, a) + p( s_3 \\mid s_1, a_1) \\cdot \\max Q(s_3, a) $$\n",
    "$$ = 0.7 \\cdot \\max Q(s_2, a) + 0.3 \\cdot \\max Q(s_3, a) $$\n",
    "\n",
    "and the value of action 2 in stage 1 as:\n",
    "\n",
    "$$ Q(s_1, a_2) = p( s_2 \\mid s_1, a_2) \\cdot \\max Q(s_2, a) + p( s_3 \\mid s_1, a_2) \\cdot \\max Q(s_3, a) $$\n",
    "$$ = 0.3 \\cdot \\max Q(s_2, a) + 0.7 \\cdot \\max Q(s_3, a) $$\n",
    "\n",
    "And finally, we use the inverse logit function to assign probabilities of taking either action 1 vs. action 2 in stage 1:\n",
    "\n",
    "$$ p( a_1 \\mid s_1, \\beta_1 ) = \\text{logit}^{-1} \\left( \\beta_1 \\cdot [Q(s_1, a_1) - Q(s_1, a_2)] \\right) = \\frac{1}{1 + \\exp \\left(-\\beta_1 \\cdot [Q(s_1, a_1) - Q(s_1, a_2)] \\right)} $$\n",
    "\n",
    "\n",
    "Note that the action probability is a function of a separate choice sensitivity (inverse temperature) parameter, $\\beta_1$. The subscript indicates that this parameter is specific to the first stage. As noted above, we typically set the value of choice sensitivity for both stages, $\\beta_1$ and $\\beta_2$, to be the same but they don't have to be!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model-Based Strategy\n",
    "\n",
    "class model_based(object):\n",
    "    def __init__(self, trials, Rs, a, b1, b2=None):\n",
    "        \n",
    "        self.n_trials = trials\n",
    " \n",
    "        self.R = Rs\n",
    "        self.state = SA\n",
    "        \n",
    "        # define parameters\n",
    "        self.alpha = a\n",
    "        self.beta1 = b1\n",
    "        \n",
    "        if b2 == None:\n",
    "            self.beta2 = b1\n",
    "        else:\n",
    "            self.beta2 = b2\n",
    "        \n",
    "        # define values\n",
    "        self.MB = np.zeros((3, 2)) # 3-states, 2 actions per state\n",
    "        # 0 = LEFT (common to SB if in level 1), 1 = RIGHT (common to SC if in level 1)\n",
    "        \n",
    "        self.rewards = np.zeros(trials)\n",
    "        self.choices = np.zeros((trials, 2))\n",
    "        self.choice1_outcomes = np.zeros(trials)\n",
    "        self.switch = np.zeros(trials)\n",
    "        self.common = np.zeros(trials)\n",
    "        \n",
    "        self.transitions = np.array([\n",
    "            [0.7, 0.3],\n",
    "            [0.3, 0.7]\n",
    "        ])\n",
    "        \n",
    "        self.transition_count = np.zeros((2,2,2))\n",
    "        self.final_state = np.zeros(trials) # 1 = SB, 2 = SC\n",
    "        \n",
    "    def possible_switch(self, choice):\n",
    "        if random.random() < 0.7:\n",
    "            return choice\n",
    "        return 1 - choice\n",
    "    \n",
    "    def update_stay_prob(self, action, t):\n",
    "        self.transition_count[\n",
    "            int(not self.rewards[t-1]),\n",
    "            int(not self.common[t-1]),\n",
    "            int(not (self.choice1_outcomes[t-1] == action))\n",
    "        ] += 1\n",
    "        \n",
    "    def compute_stay_prob(self, transition_count):\n",
    "        # stay_prob[r,c,a] = P[r,c,a] / (P[r,c,a] + P[r,c,~a]) \n",
    "        action_count = transition_count.sum(axis=-1)\n",
    "        return transition_count / action_count[:, :, np.newaxis]\n",
    "\n",
    "    def train(self, R):\n",
    "        \n",
    "        for t in range(self.n_trials):\n",
    "            \n",
    "            self.state = SA\n",
    "            \n",
    "            ## Action selection.\n",
    "            self.MB[self.state, 0] = 0.7*max(self.MB[1]) + 0.3*max(self.MB[2])\n",
    "            self.MB[self.state, 1] = 0.3*max(self.MB[1]) + 0.7*max(self.MB[2])\n",
    "            \n",
    "            # choice probabilities and making choice\n",
    "            theta1 = inv_logit( self.beta1 * (self.MB[self.state, 0] - self.MB[self.state, 1]) )\n",
    "            choice1 = np.random.binomial(1, theta1)\n",
    "            self.choices[t,0] = choice1\n",
    "            \n",
    "            # observe outcome and possible switch\n",
    "            # IS THIS CORRECT? Or is the switch already accounted for??\n",
    "            outcome1 = self.possible_switch(choice1)\n",
    "            self.choice1_outcomes[t] = outcome1\n",
    "            \n",
    "            # update values for stay_probs\n",
    "            if t > 0:    \n",
    "                self.update_stay_prob(outcome1, t)\n",
    "            \n",
    "            reward_probs = None\n",
    "            # update state\n",
    "            if self.choice1_outcomes[t] == 0: # went LEFT\n",
    "                self.state = SB\n",
    "                self.final_state[t] = 1\n",
    "                reward_probs = self.R[t][0:2]\n",
    "            else: # went RIGHT\n",
    "                self.state = SC\n",
    "                self.final_state[t] = 2\n",
    "                reward_probs = self.R[t][2:4]\n",
    "            \n",
    "            # count possible switch\n",
    "            if (self.choices[t,0] == 1 & self.state == SB) | (self.choices[t,0] == 0 & self.state == SC):\n",
    "                self.switch[t] = 1\n",
    "            else:\n",
    "                self.common[t] = 1\n",
    "            \n",
    "            # make second-level choice\n",
    "            d2 = self.MB[self.state,0] - self.MB[self.state,1]\n",
    "\n",
    "            theta2 = inv_logit( self.beta2 * d2 )\n",
    "            choice2 = np.random.binomial(1, theta2)\n",
    "            self.choices[t,1] = choice2\n",
    "            \n",
    "            # get what the reward is\n",
    "            final_prob = reward_probs[choice2]\n",
    "            reward = np.random.binomial(1, final_prob)\n",
    "            self.rewards[t] = reward\n",
    "            \n",
    "            # update values\n",
    "            # update level 2 MB value\n",
    "            self.MB[self.state, choice2] += self.alpha*(self.rewards[t] - self.MB[self.state, choice2])\n",
    "            \n",
    "            # update level 1 MB value\n",
    "            self.MB[SA, outcome1] += self.alpha*(self.rewards[t] - self.MB[self.state, choice2])\n",
    "            \n",
    "    def plot(self, transition_count=None, title=\"Model-Based Two-Step Task\", y_lim=0.5):\n",
    "        _,ax = plt.subplots(1,1,figsize=[10,6])\n",
    "\n",
    "        ax.set_ylim([y_lim, 1.0])\n",
    "        ax.set_ylabel('Stay Probability')\n",
    "        ax.set_title(title)\n",
    "\n",
    "        if transition_count is None:\n",
    "            transition_count = self.transition_count\n",
    "        \n",
    "        stay_probs = self.compute_stay_prob(transition_count)\n",
    "        \n",
    "        common = [stay_probs[0,0,0], stay_probs[1,0,0]]\n",
    "        uncommon = [stay_probs[0,1,0], stay_probs[1,1,0]]\n",
    "        \n",
    "        ax.set_xticks([1.5,3.5])\n",
    "        ax.set_xticklabels(['Rewarded', 'Unrewarded'])\n",
    "        ax.set_ylim(0,1)\n",
    "        \n",
    "        c = plt.bar([1,3], common, color='b', width=0.5)\n",
    "        uc = plt.bar([2,4], uncommon, color='r', width=0.5)\n",
    "        ax.legend( (c[0], uc[0]), ('Common', 'Uncommon') )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_agents = 1\n",
    "trials = 400\n",
    "alpha = 0.5\n",
    "beta = 10.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(11408)\n",
    "agents_MB = model_based(trials, Rs, alpha, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "MBtrained = agents_MB.train(Rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "agents_MB.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS WRONG!! â€”â€” something wrong here with the way that you are coding the task . . . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "# would this be better if we used two betas, one for each stage, like Daw and Toyama did?\n",
    "# is this the right way to calculate Stay Probabilities? This seems low when you look at the graph . . . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
